# AlphaZero-style NÃ—N K-in-a-row Agent

A general NÃ—N K-in-a-row game agent that learns to play optimally via self-play, guided by a policy/value neural network and Monte Carlo Tree Search (MCTS).

## ğŸ¯ Project Overview

This project implements an AlphaZero-inspired reinforcement learning agent that can:
- Play generalized tic-tac-toe on any NÃ—N board
- Learn optimal strategies through self-play
- Scale from 3Ã—3 standard tic-tac-toe to larger boards like 5Ã—5 with K=4

## ğŸ“ Project Structure

```
tictactoeMCTS/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ env/          # Game environment
â”‚   â”œâ”€â”€ mcts/         # Monte Carlo Tree Search
â”‚   â”œâ”€â”€ net/          # Neural network architecture
â”‚   â”œâ”€â”€ trainer/      # Training pipeline
â”‚   â””â”€â”€ eval/         # Evaluation and agents
â”œâ”€â”€ tests/            # Unit tests
â”œâ”€â”€ notebooks/        # Jupyter notebooks for analysis
â”œâ”€â”€ data/            # Training data and models
â”œâ”€â”€ play_cli.py      # Command-line game interface
â””â”€â”€ requirements.txt # Python dependencies
```

## ğŸš€ Getting Started

### Installation

1. Clone the repository:
```bash
cd tictactoeMCTS
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Play the Game

```bash
# Play 3Ã—3 tic-tac-toe against random agent
python play_cli.py

# Play 5Ã—5 with 4-in-a-row against another human
python play_cli.py -n 5 -k 4 -p1 human -p2 human

# Watch random agents play (for testing)
python play_cli.py -p1 random -p2 random
```

### Run Tests

```bash
# Run all tests
python -m unittest discover tests/

# Run with coverage (if pytest-cov installed)
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“š Week 1 Milestone (Completed) âœ…

- âœ… Implemented generalized `GameEnv` class for NÃ—N K-in-a-row
- âœ… Board representation using NumPy arrays
- âœ… Win detection (horizontal, vertical, diagonal, anti-diagonal)
- âœ… Draw detection and edge cases
- âœ… Board encoding for neural network input (one-hot planes)
- âœ… Comprehensive unit tests (21 tests, all passing)
- âœ… CLI for human vs random gameplay
- âœ… Project structure and dependencies setup

## ğŸ® Game Environment Features

The `GameEnv` class supports:

- **Configurable board size**: Any NÃ—N board
- **Flexible win condition**: K pieces in a row (K â‰¤ N)
- **Player representation**: 1 (X), -1 (O), 0 (empty)
- **Win detection**: All directions (horizontal, vertical, both diagonals)
- **Neural network encoding**: 3-plane one-hot encoding
- **Canonical board**: Current player perspective for training
- **Game state cloning**: For MCTS simulations

### Example Usage

```python
from src.env.game_env import GameEnv

# Create 3Ã—3 tic-tac-toe
env = GameEnv(n=3, k=3)

# Make moves
board, reward, done = env.apply_move(0, 0)  # X at (0,0)
board, reward, done = env.apply_move(1, 1)  # O at (1,1)

# Get board encoding for neural network
encoding = env.get_board_encoding()  # Shape: (3, N, N)

# Check legal moves
legal_moves = env.get_legal_moves()  # List of (row, col) tuples

# Display board
print(env.render())
```

## ğŸ§ª Testing

The test suite covers:
- Basic game mechanics (initialization, reset, move validation)
- Win detection in all directions
- Draw conditions
- Edge cases (different board sizes, K values)
- Board encoding correctness
- Game state cloning

All 21 tests pass successfully.

## ğŸ“‹ Next Steps (Upcoming Weeks)

- **Week 2**: Neural network architecture & supervised learning
- **Week 3**: MCTS implementation
- **Week 4**: Self-play pipeline
- **Week 5**: Training stability & tuning
- **Week 6**: Scaling to larger boards
- **Week 7**: Demo & documentation

